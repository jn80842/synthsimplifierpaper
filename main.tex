%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}


\usepackage{amsmath}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{csvsimple}
\usepackage{microtype}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{algorithm2e}
\usetikzlibrary{hobby,arrows,backgrounds,calc,trees}

\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

\newcommand{\convexpath}[2]{
[   
    create hullnodes/.code={
        \global\edef\namelist{#1}
        \foreach [count=\counter] \nodename in \namelist {
            \global\edef\numberofnodes{\counter}
            \node at (\nodename) [draw=none,name=hullnode\counter] {};
        }
        \node at (hullnode\numberofnodes) [name=hullnode0,draw=none] {};
        \pgfmathtruncatemacro\lastnumber{\numberofnodes+1}
        \node at (hullnode1) [name=hullnode\lastnumber,draw=none] {};
    },
    create hullnodes
]
($(hullnode1)!#2!-90:(hullnode0)$)
\foreach [
    evaluate=\currentnode as \previousnode using \currentnode-1,
    evaluate=\currentnode as \nextnode using \currentnode+1
    ] \currentnode in {1,...,\numberofnodes} {
  let
    \p1 = ($(hullnode\currentnode)!#2!-90:(hullnode\previousnode)$),
    \p2 = ($(hullnode\currentnode)!#2!90:(hullnode\nextnode)$),
    \p3 = ($(\p1) - (hullnode\currentnode)$),
    \n1 = {atan2(\y3,\x3)},
    \p4 = ($(\p2) - (hullnode\currentnode)$),
    \n2 = {atan2(\y4,\x4)},
    \n{delta} = {-Mod(\n1-\n2,360)}
  in 
    {-- (\p1) arc[start angle=\n1, delta angle=\n{delta}, radius=#2] -- (\p2)}
}
-- cycle
}
\tikzset{
  leaf/.style={sibling distance=5mm}
}

\definecolor{uwpurple}{RGB}{128,0,128}
\newcommand{\jln}[1]{\textcolor{uwpurple}{\textit{[{#1} --JLN]}}}
\newcommand{\sak}[1]{\textcolor{olive}{\textit{[{#1} --SK]}}}
\newcommand{\aba}[1]{\textcolor{red}{\textit{[{#1} --AA]}}}
\usepackage{syntax}
%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2018}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Macros for quantities
\newcommand{\NumApps}{{\color{black} 10}\xspace}
\newcommand{\NumRulesFixed}{{\color{black} 4}\xspace}
\newcommand{\NumPredicatesRelaxed}{{\color{black} 17}\xspace}
\newcommand{\NumOrderingProblems}{{\color{black} 46}\xspace}
\newcommand{\NumRulesSynthesized}{{\color{black} 2632}\xspace}
\newcommand{\NumOpSequences}{{\color{black} 6246}\xspace}
\newcommand{\NumFailureExamples}{{\color{black} 61000}\xspace}
\newcommand{\NumSimplifiedExpressions}{{\color{black} 195371}\xspace} 
\newcommand{\NumBugsAutomated}{{\color{black} 5}\xspace}
\newcommand{\NumOriginalRules}{{\color{black} 999}\xspace}
\newcommand{\NumZdivRules}{{\color{black} 1139}\xspace}
\newcommand{\NumZdivZThreeProvedRules}{{\color{black} 988}\xspace}
\newcommand{\NumZdivCoqProvedRules}{{\color{black} 141}\xspace}
\newcommand{\NumZdivFalseRules}{{\color{black} 10}\xspace}
\newcommand{\NumZdivRelaxedPredicates}{{\color{black} 37}\xspace}


%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption


\begin{document}

%% Title information
\title{Verifying and Improving Halideâ€™s Term Rewriting System with Program Synthesis}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
  Halide is a domain-specific language for high-performance image processing and tensor computations, widely adopted in industry.
  Internally, the Halide compiler relies on a term rewriting system to
  prove properties of code required for efficient and correct compilation.  This rewrite
  system is a collection of handwritten transformation rules 
  that incrementally rewrite expressions into simpler forms; the system requires high performance 
  in both time and memory usage to keep compile times low.
  In this work, we apply formal techniques to prove the correctness of existing
  rewrite rules and provide a guarantee of termination. Then,
  we build an automatic program synthesis system that operates over the undecidable theory
  of integers in order to craft new, provably correct rules from failure cases
  where the compiler was unable to prove properties. We identify and fix
  \NumRulesFixed incorrect rules as well as \NumOrderingProblems rules which
  could give rise to infinite rewriting loops.
  %% We run our synthesis process against 
  %% a large suite of failed proofs from compiler output, synthesizing a new body
  %% of rules that improves the system's proving power as evaluated on realistic
  %% benchmarks.
  %% The automated system can replace tedious human effort required to craft new rules
  %% in response to user-reported bugs.  We demonstrate that the new
  %% term rewriting system is provably sound, more robust, and more general, without
  %% slowing down the compiler.
  We then demonstrate the utility of our verification and synthesis framework using XX case studies,
  including automatically fixing term rewriting bugs and verifying and assisting developers
  when making large-scale changes to the Halide compiler.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
%\keywords{keyword1, keyword2, keyword3}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}
Tensor and image processing pipelines power a large number of applications,
including computational photography, medical imaging, machine learning,
and computer vision, which run on a large variety of platforms, from
mobile phones to large-scale servers in the cloud.  For many of these
applications, high performance is an essential requirement due to real-time
requirements, compute cost, and battery life.

Halide~\cite{ragankelley2012halide, ragankelley2013halide} has proven to be an effective
domain-specific language for writing such code and obtaining cross-platform
high performance, with multiple industrial vendors relying on Halide to power
applications used by millions of users on mobile phones, tablets, desktops, and
servers.  Halide separates what is computed (the \textit{algorithm}) from how
the computation executes (the \textit{schedule}); the compiler ensures that
different schedules for different platforms correctly compute the algorithm.
Schedules often require the compiler to prove specific properties in order
to guarantee correctness or to ensure the resulting code performs as well
as possible.  For example, to fully unroll a loop, the compiler must prove
the loop has a constant extent.

Many of these properties relate regions of computation with one another,
which, within Halide, are represented as intervals over the integers.
Reasoning about these regions uses a \textit{term rewriting system}
(TRS): the compiler applies simplification rules to an expression relating
the regions until no more rewrites apply.  The final expression may be
true, false, or some more complex expression. If an expression is semantically equivalent to true,
but the TRS cannot rewrite it to that form, the compiler may be unable to apply a valid optimization,
resulting in suboptimal code output and possibly errors during compilation.

In this work, we improve the Halide term rewriting system by formally checking the existing
set of rules to ensure correctness, and then constructing a rule rewrite strategy that 
ensures termination.  We then automate the task of writing new rules by building a program
synthesis system that automatically checks proof failures (expressions that are
correct but not provable by the current TRS), and produces new rules with predicates to guard their application.
These synthesized rules preserve correctness and termination properties by construction.

An ideal TRS for our purpose would fully express the semantics of the Halide
expression language and be \emph{convergent}, meaning that if two expressions
are equivalent, the TRS will return the same canonical form for both of them. If
so, the rewriter could take any two expressions and determine their equivalence
 by rewriting them and checking to see if they have the same form.
However, since the Halide expression language contains nonlinear integer
arithmetic, the problem of constructing such a term rewriting system is
undecidable~\cite{matiyasevich1993hilberts10th}. Thus, the challenge for the
Halide TRS is to reason about as much of the Halide expression space
as it can, while remaining deterministic and performant.

The Halide term rewriting system can be called thousands of times per
compilation, so the rewriter must be exceedingly fast.  The Halide TRS consists of a series of handwritten rules and efficient matchers
to traverse expressions bottom-up. In order to be as robust as possible, the Halide TRS is structured
to scale well with the size of the ruleset; currently, it consists of nearly a thousand
rewrite rules.  These rules were constructed by developers based on
observed compilation failures over many years, and ordered in ad-hoc ways to
avoid non-termination. Extensive fuzz-testing has been able to find bugs in
rewrite rules, but fuzz-testing cannot provide formal guarantees (and in fact we
were able to find bugs in the current version of the term rewriter;
see Section~\ref{sec:eval-correctness}). Furthermore, adding additional rules requires
expert programmers to meticulously analyze compilation failures and handcraft
rules that mitigate the problems without causing new errors, which is a process
that is notoriously difficult and the subject of numerous complaints from
Halide developers.

Verification and synthesis of Halide term rewriting rules poses significant
challenges. For example, while prior work~\cite{lopes2015alive,menendez2017aliveinfer}
considered verification and synthesis over the decidable theory of bitvectors,
Halide's semantics require our machinery to operate over the undecidable theory
of integers.
We verify existing rules using a combination of the Z3~\cite{de2008z3}
satisfiability modulo theory (SMT) solver and the Coq~\cite{coq19} interactive
theorem prover. New rules are constructed by combining Z3 and a handwritten
solver that uses beam search with a learned cost model. The synthesis process learns rules from a training set of expressions, which can be gathered by instrumenting the Halide compiler to log expressions for which the existing ruleset fails.

We make three main contributions:
\begin{itemize}
  \item We guarantee soundness by verifying the existing Halide TRS.
  \item We guarantee termination by defining a rule strategy that prevents rewriter cycles.
  \item We develop a novel synthesis algorithm over the undecidable theory of integers to learn new rewrite rules from
    expressions the current rewriter cannot make progress on.
\end{itemize}

We demonstrate the utility of our framework by using it to assist in
    fixing bugs and performing large-scale changes in the Halide compiler, which would otherwise require massive
    investment of human effort to ensure correctness.

\jln{this next para is outdated and should be updated to reflect current eval section}
Overall, we discover \NumRulesFixed bugs in handwritten Halide rewrite rules, and discover \NumOrderingProblems
potential issues in the rule rewrite strategy.  We then synthesize \NumRulesSynthesized new rules, use the
synthesis framework to maintain correctness when modifying compiler semantics,
and further demonstrate the utility of our synthesizer by 
automatically reproducing the \NumBugsAutomated latest code changes that add new
rules based on compiler misbehavior.
%We additionally craft a strategy for grouping
%new rules that prevents compile times from increasing linearly with the number
%of rules.
Overall, we demonstrate that our approach
reduces failed proofs,  does not slow down the compiler, and replaces the
tedious human effort required to craft bug fixes due to missing rules in the
term rewriting system.

\section{Background \& Overview}
\subsection{Term Rewriting Systems}
Term rewriting systems~\cite{gorn1967} are sets of \textit{rewrite rules} used to transform expressions into a new form.  Such systems are widely
used in theorem proving~\cite{baader1999term} and abstract interpretation~\cite{cousot1977abstract, cousot1979systematic}.

Terms are defined inductively over a set of variables $V$ and a set of function symbols $\Sigma$. Every variable $v \in V$ is a term, and for any function symbol $f \in \Sigma$ with arity $n$ and any terms $t_1, ..., t_n$, the application of the symbol to the terms $f(t_1, ..., t_n)$ is also a term. (Constants are considered zero-arity functions.) We refer to the set of terms constructed from the variables $V$ and the function symbols $\Sigma$ as $T(\Sigma, V)$.

A \emph{rewrite rule} is a directed binary relation $l \rightarrow_R r$ such that $l$ is not a variable, and all variables present in $r$ are also present in $l$ (i.e., $\mathcal{V}ar(l) \supseteq \mathcal{V}ar(r)$). A set of rewrite rules is a \emph{term rewriting system}.

Consider a set of terms $T(\Sigma, V)$ such that $\Sigma = \{\clubsuit, \diamondsuit\}$ and $V$ is an infinite set of variables. Let the term rewriting system $R$ consist of a single rule:

\[ R = \{ x_1 \clubsuit x_2 \rightarrow_R x_1 \diamondsuit x_2 \} \]

We use $R$ to rewrite the term

\[ 
(y_1 \diamondsuit y_1) \clubsuit (y_2 \clubsuit y_3)
\]

The first step is matching; we find a substitution that will unify the left-hand side (LHS) of the rule with the term we are rewriting. Here, one possible substitution is:

\[
\{ x_1 \mapsto (y_1 \diamondsuit y_1), x_2 \mapsto (y_2 \clubsuit y_3) \}
\]

We then apply this substitution to the right-hand side (RHS) of the rule to obtain the rewritten version of the original term.

\[ 
(y_1 \diamondsuit y_1) \diamondsuit (y_2 \clubsuit y_3)
\]



\begin{figure*}
\begin{tabular}{cccc}

%\Tree [.+ [.- [.min a b ] [.max c c ] ] [.max c c ]]
\begin{tikzpicture}[level distance=12mm]
\tikzstyle{level 1}=[sibling distance=15mm]
\tikzstyle{level 2}=[sibling distance=10mm]
\tikzstyle{level 3}=[level distance=10mm,sibling distance=5mm]


%\Tree [.+ [.- [.min a b ] c ] [.max c c ]]
\node (+) {+}
  child { node (-) {-}
    child { node (min) {min}
      child {node (a) {a}}
      child {node (b) {b}}
    }
    child { node (max2) {max}
      child {node (c4) {c}}
      child {node (c5) {c}}
    }
  }
  child { [sibling distance=5mm] node (max) {max}
    child { node (c2) {c}}
    child { node (c3) {c}}
  };
\begin{pgfonlayer}{background}
\fill[red,opacity=0.3] \convexpath{c4,max2,c5}{10pt};
\end{pgfonlayer}
\end{tikzpicture}

&
\begin{tikzpicture}[level distance=12mm]
\tikzstyle{level 1}=[sibling distance=15mm]
\tikzstyle{level 2}=[sibling distance=7mm]
\tikzstyle{level 3}=[level distance=10mm,sibling distance=5mm]
%\Tree [.+ [.- [.min a b ] c ] [.max c c ]]
\node (+) {+}
  child {  node (-) {-}
    child { node (min) {min}
      child {node (a) {a}}
      child {node (b) {b}}
    }
    child { node (c) {c}}
  }
  child { node (max) {max}
    child { node (c2) {c}}
    child { node (c3) {c}}
  };
\begin{pgfonlayer}{background}

\fill[blue,opacity=0.3] \convexpath{c2,max,c3}{10pt};
%\fill[red,opacity=0.3] \convexpath{c}{10pt};
\draw[red,fill=red,opacity=0.3](c.north) to[closed,curve through={($(c.north east)!1.0!(c.south east)$) .. ($(c.south west)!1.0!(c.north west)$)}] (c.north);

\end{pgfonlayer}
\end{tikzpicture}
&
\begin{tikzpicture}[level distance=12mm]
\tikzstyle{level 1}=[sibling distance=15mm]
\tikzstyle{level 2}=[sibling distance=10mm]
\tikzstyle{level 3}=[level distance=10mm,sibling distance=5mm]
%\Tree [.+ [.- [.min a b ] c ] c ]
\node (+) {+}
  child { node (-) {-}
    child { node (min) {min}
      child {node (a) {a}}
      child {node (b) {b}}
    }
    child { node (c) {c} }
  }
  child { node (c2) {c}};



  \begin{pgfonlayer}{background}
\fill[green,opacity=0.3] \convexpath{b,a,min,-,+,c2,c}{10pt};
\draw[red,fill=blue,opacity=0.3](c2.north) to[closed,curve through={($(c2.north east)!1.0!(c2.south east)$) .. ($(c2.south west)!1.0!(c2.north west)$)}] (c2.north);
\end{pgfonlayer}
\end{tikzpicture}
&
\vspace{0pt}
\begin{tikzpicture}[level distance=12mm]
\tikzstyle{level 1}=[sibling distance=15mm]
\tikzstyle{level 2}=[sibling distance=10mm]
\tikzstyle{level 3}=[level distance=10mm,sibling distance=5mm]
\node (min) {min}
  child { node (a) {a}
     % [red,opacity=0.0]
    child {     [red,opacity=0.0] node (fake1) {f}
      child {    [red,opacity=0.0] node (fake2) {f}}
      child {    [red,opacity=0.0] node (fake3) {f}}
    }
    child {     [red,opacity=0.0] node (fake4) {f}
    [red,opacity=0.0]
    child { [red,opacity=0.0] node (fake) {f}}
    child { [red,opacity=0.0] node (fake2) {f}}
    }
  }
  child { node (b) {b}
     % [red,opacity=0.0]
    child {     [red,opacity=0.0] node (fake5) {f}}
    child {     [red,opacity=0.0] node (fake6) {f}}
  };
  \begin{pgfonlayer}{background}
\fill[green,opacity=0.3] \convexpath{a,min,b}{10pt};
\end{pgfonlayer}

\end{tikzpicture} \\
(i) & (ii) & (iii) & (iv)
\end{tabular}
\caption{We demonstrate the Halide rewriting algorithm using a TRS $R = \{max(x,x) \rightarrow_R x, (x - y) + y \rightarrow_R x\}$ and an expression $min(a,b) - max(c,c) + max(c,c)$. The algorithm attempts to simplify all subtrees bottom up; here, no rule applies to $min(a,b)$ so it is not changed. Next (i), rule 1 rewrites $max(c,c)$ to $c$. No rule applies to $min(a,b) - c$, so we move to the rightmost subtree and rewrite again (ii) to obtain $c$ from $max(c,c)$. Finally, we consider the entire tree $min(a,b) - c) + c$ (iii) and apply rule 2 to produce $min(a,b)$. No rules match this expression, so we are left with $min(a,b)$ (iv).}
\label{fig:algoexample}
\end{figure*}

\subsection{Term Rewriting in Halide}

The Halide compiler contains a term rewriting system, currently composed of
over a thousand rules, that operates over the space of Halide
expressions\footnote{See Supplemental Material for the full Halide
  expression grammar}, a relatively small language of integer, boolean, and vector functions. While the term rewriting system is used for multiple
purposes within the compiler, the two most important
uses are as a proof engine and as an expression simplifier to aid in bounds inference. For example, in the first case, in order to parallelize a
reduction variable, the compiler must prove that there are no hazards, which is
done by asking the TRS to determine if some equalities or inequalities hold.  As an example of the second, Halide uses
the TRS to decide how much memory to allocate for intermediates; tighter, more
simplified bounds result in more efficient execution since they eliminate over-allocation.
More generally, proof engine failures can lead to:
\begin{itemize}
\item Insufficiently tight bounds on loops and allocations, which may result in
  runtime failures (e.g. due to memory overallocation) or performance issues;

\item Failure of the compiler to apply optimizations, also resulting in slow performance;

\item Dynamic checks in the generated code for properties that could have been proven
  at compile time, leading to slower code;

\item Compilation failures, when the compiler is unable to correctly produce code
  even though the properties required hold, or when the proof engine itself crashes
  or loops infinitely.
\end{itemize}
Thus, correctness and generality are essential to make the compiler robust and
able to generate fast code.

The design of the term rewriting algorithm has two additional important
requirements. The first is performance: although the TRS is called at
compile time, it may be invoked over a hundred thousand times in compiling a single pipeline. 
The Halide TRS
never backtracks and maintains only the expression currently being rewritten as state. The second requirement is determinism: the compiler must
always return the same schedule every time it encounters a particular pipeline.
This precludes any non-deterministic search strategy for the best sequence of
rules to apply, or directly invoking a solver such as Z3, since even with a
fixed random seed, machines with different computational power may return
different results within a given timeout.

The Halide term rewriting algorithm simplifies an input expression in a
depth-first, bottom-up traversal of the expression DAG. At each node, it looks
up the list of rewrite rules that correspond to the node's function symbol, then
attempts to match the subtree expression with the rule LHSs in order. Matching is performed purely syntactically, using C++ templates, without any AC-matching or more sophisticated data structures. When a
match is found, it rewrites the subtree expression using the RHS of that rule,
and then attempts to simplify the subtree expression again. If no rule matches
the subtree, the traversal continues; when the entire expression cannot be
simplified further, the rewritten expression is returned. See Figure~\ref{fig:algoexample} for a worked example. 

Halide rewrite rules optionally contain a compile-time predicate that must hold for a rule to
be applied. Halide expressions may contain constants whose values are known at
compile time. Predicate expressions contain only such constants\footnote{Existing rules sometimes have predicates that check if non-constant variables can be shown to have certain properties at compile thing, but these are expensive and used sparingly.}; when the
LHS of a rule matches an expression, its predicate is evaluated and only if it
is true will the rewrite be applied.

We limit ourselves to expressions over the set of infinite precision integers in
this work. Although Halide expressions can contain floats and fixed-width
integers, the uses of the TRS we consider in this work all operate
on infinite integers.

\subsection{Why a Custom Algorithm?}

Given that we make use of the Z3 solver as an oracle for both verification and synthesis, it is natural to ask why Halide could not simply call Z3 for simplification. Z3 is the product of extensive development and is a very powerful, general-purpose solver. However, the Halide term rewriting system has a few key properties that Z3 does not: deterministic output, low memory and compute requirements, and domain-specific optimizations.

As discussed above, Halide compiler must return the same schedule every time the same pipeline is run. Z3 can fix a random seed, but long-running queries may complete on a more powerful server while timing out on a different machine.

During rewriting, a single rule may be able to match an input expression in multiple ways, and there may be multiple rules in the ruleset which could be used to rewrite the expression. A term rewriting algorithm might choose one of many alternatives and later backtrack if it turns out not to be fruitful; it might make use of heuristics to choose a next step; it might exercise all the alternatives and keep the results in equivalence classes, as in an egraph. The Halide TRS algorithm proceeds in an defined order, rewriting expressions from the bottom up, and attempting to match rules one after another in a fixed priority, applying the first rule that matches. This is very fast and requires very little memory; the tradeoff is that the algorithm may pick the ``wrong'' rule and have no way of undoing that decision. Since the rewriter is invoked many thousands of times with each call of the compiler, it chooses to sacrifice some solving power in exchange for performance.

This choice gives the Halide algorithm a very smooth performance curve, whether it succeeds or fails in simplifying an input expression, while a solver like Z3 tends to give very good performance in the majority of cases but gets bogged down in difficult cases, requiring the use of timeouts. The Halide algorithm ``fails fast'': on an input expression which cannot be matched by any rule in the ruleset; the Halide algorithm will complete in time linear to the size of the expression. To demonstrate this performance tradeoff, we gathered 4304 expressions from queries the Halide compiler made when compiling realistic pipelines, including both expressions that are provably true and expressions that are not provably true, and checked them with Z3 (using a 60 second timeout) and the Halide TRS. Z3 could prove more expressions true, but was starkly less performant. As shown in Table~\ref{tab:simplifiervsz3}, Z3 took over seven minutes to check the set of expressions while the Halide TRS took just two seconds. The set of expressions is much smaller than the number of calls the compiler would make to the rewriter in the compilation of a single pipeline.

\begin{table}
\caption{We compare the performance of Z3 and the Halide TRS in proving a set of 4304 expressions gathered from realistic compiler output.}
\begin{tabular}{l|r|r|r}
Tool & Runtime & Proven expressions & Not proven \\
\hline
Z3 & 7m29s & 1125 & 3179 \\
Halide TRS & 2s & 885 & 3419 
\end{tabular}
\label{tab:simplifiervsz3}
\end{table}

Because the Halide algorithm at every step chooses one rule to apply to the single expression it is working on, it scales well in terms of the number of rules in the TRS. See Section~\ref{ssec:compilationspeed} for an evaluation of the effects of adding newly synthesized rules on the performance of the compiler. 

Finally, although Z3 can be used to simplify expressions, simply reducing the size of an expression is not necessarily the goal for the compiler. For example, gathering like terms in some cases can actually prevent Halide or LLVM optimizations from applying. The Halide rewriter uses a domain-specific strategy to guide expressions into more optimizable forms and can be changed or tuned as needed if further optimizations are discovered. 

%Z3 is general-purpose, but we need not be. The handwritten simplifier ruleset was written to address expressions that are generated by the Halide compiler. Since we are driven by a subset of all Halide expressions and not arbitrary expressions in the Halide expression language, we can specialize and in some cases solve expressions that Z3 cannot. 


%Egraphs grow their equivalence classes with each invocation of their rulesets until they converge or until their graph grows so large that progress is no longer possible. Since we can't guarantee that our rules converge, the problem is reaching the goal state before the growth of the graph reaches that infeasible inflection point.

%Egraph handling of predicates is also more computationally expensive than the Halide TRS; Halide evaluates constants at compile time to see if a predicate obtains on a particular expression. Since egraphs have no knowledge of semantics, the machinary for handling predicates is much more involved.

%If bounded, egraphs are also deterministic, so that's not a problem.

%I know that dumping the Halide ruleset into an egraph is completely unworkable, since I tried it -- not a fair comparsion since you would tune an egraph's ruleset differently.

%The Halide TRS gives a nice starting point for synthesis because a failed call to the simplifier yields an expression the simplifier can make no progress on. A failed egraph call gives an equivalence class that should include the goal state but doesn't, so it's not as clear how to debug that.

\subsection{Program Synthesis}
Our pipeline for automatically generating new rules relies on program synthesis.
Syntax-guided synthesis (SyGuS)~\cite{sygus} searches a space of possible programs in order to find one that meets
a semantic specification.  One of the most popular techniques is counterexample-guided
inductive synthesis (CEGIS), which leverages the power of satisfiability modulo theories
(SMT) and SAT solvers to make search over a large space of programs more intelligent.
The task is to find some program in the search space such that for all possible
inputs, the found program fulfills some semantic criteria, usually equivalence with
a specification.

The CEGIS loop alternates between a learning query, which finds a new candidate program over a small set of inputs, and a verification query which checks to see if the program satisfies the specification for all inputs. We begin with a specification $\phi$, choose an initial input $i_0$, and issue a learning query to find a candidate program:

$$\exists P . \phi(P(i_0))$$

When some program $P_1$ is found, we ask a verification query to see if there is some input on which $P_1$ does not fulfill the specification:

$$\exists i . \neg \phi(P_1(i))$$

If no such input can be found, the program satisfies the specification on all inputs and we are done. If not, the query returns some $i_1$ as a counterexample. We then return to our learning query, enhanced with the newly found counterexample:

$$\exists P . \phi(P(i_0)) \wedge \phi(P(i_1))$$

We continue in this way until either a program is found for which the verification query can find no counterexample, or until the learning query cannot find a program that fulfills the specification when applied to all of the learned counterexample set.

\section{Soundness}
\label{sec:soundness}

We improve the Halide term rewriting system by ensuring its soundness in
two ways: first, we verify that each individual rule is correct, meaning that the
rewrite preserves semantics. Then we verify that the term rewriting system is
guaranteed to terminate on all inputs by ensuring that there is no sequence of
applications of rules, on any input expression, that can form a cycle.

\subsection{Rule verification}

We verify each individual rule is correct by modeling Halide
expressions in SMT2 and using the SMT solver Z3~\cite{de2008z3} to
prove that the rule equivalences must hold.  Most Halide expression
semantics map cleanly to SMT2 formulas. The functions \texttt{max} and
\texttt{min} are defined in the usual way, and \texttt{select} in
Halide is equivalent to the SMT2 operator \texttt{ite}. Division and
modulo are given the Euclidean definitions in both Halide and
SMT2~\cite{boute1992euclidean}, though division by zero is handled
differently (in Halide both evaluate to zero).
%If a variable appears in the LHS of a rule as a divisor in a
%division or modulo operation, it is assumed to be non-zero. %The Halide
%expressions do not have a true boolean type (true and false are represented by
%unsigned integers of 1 bitwidth), so expressions must be typed as either
%\texttt{Int} or \texttt{Bool} when translated into SMT2. The Halide expression
Halide's TRS uses two vector-only operators, \texttt{broadcast} and \texttt{ramp}; all
other integer operators can be coerced to vector operators. 
\texttt{broadcast} projects some value $x$ to a vector of length $l$; because of
the type coercion, we can simply represent \texttt{broadcast(x)} as the variable
\texttt{x} in SMT2. \texttt{ramp} creates a vector of length $l$
whose initial entry has the value $x$ and all subsequent entries increase with
stride $s$. In SMT2, we represent this term as the symbolic expression $x + l *
s$, where $l$ must be zero or positive.

Given this modeling, for each rule, we assert any assumptions are true, then
ask Z3 to search for a variable assignment such that the LHS and RHS are not
equivalent.  If Z3 indicates no such assignment exists, the LHS must be equivalent to
the RHS and the rule must be correct. We implemented an SMT2 printer for 
Halide rewrite rules that constructs a verification problem for each rule
suitable for passing to Z3 or other SMT system.  Rule verification using Z3 is fully automated
and can be run for the current set of rewrite rules via script.

However, for 123
rules, Z3 either timed out or returned unknown. Nearly all of these rules used
either division or modulo. We used the proof assistant Coq to manually prove the correctness of these remaining rules. In the course of these proofs, we discovered we
were also able to relax the predicates of \NumPredicatesRelaxed rules; for example, in some cases a rule
with a predicate requiring some constant to be positive would be equally valid
if the constant was non-zero.

%
%We further want to guarantee that, given an input expression with well-defined
%behavior, the term rewriting system will never rewrite it into an expression that has
%undefined behavior. Thus, if an input expression never divides by zero, then the
%TRS output must also never divide by zero. We check this by
%verifying that all terms used as divisors or moduluses in the RHS of a
%rewrite rule also appear as a divisor in the rule's LHS. Note that all divisor
%terms on the LHS side are not required to appear in the RHS; if a divisor is
%factored out, then it is possible that the input expression would have caused a
%divide-by-zero fault but the simplified expression will not.

\subsubsection{Evolving semantics}

This mostly-automated approach to verification assists with changing
the language semantics. Our initial work on verification was not on
the semantics descibed above. It used to be the case that division or
modulo by zero was considered undefined behavior. Since we had already
modeled Halide semantics in SMT2, it was a simple matter to alter the
definitions of division and modulo and run the verification scripts
again. At the time we ran our second verification, there were
\NumZdivRules in the Halide TRS ruleset, and \NumZdivZThreeProvedRules
were proven true in Z3. We proved \NumZdivCoqProvedRules rules to be
true manually in Coq; since in the previous round all Coq proofs
included the assumption that all divisors were non-zero, in most cases
we had only to add a case to show that the rule was true both when the
divisor was zero and when it was non-zero. In the course of reviewing
these proofs, we identified \NumZdivRelaxedPredicates rules whose
predicates included the condition that a divisor be non-zero and where
that condition could safely be lifted. We found that
\NumZdivFalseRules rules were not correct under the new semantics and
submitted a patch to amend them.

Overall results of verifying rule correctness are described in Section~\ref{sec:eval-correctness}.

\subsection{Termination}

Term rewriting systems are not guaranteed to terminate. Consider a term
rewriting system containing only one rule: $x + y \rightarrow y + x$. The term
$3 + 5$ matches the LHS of the rule and is rewritten to $5 + 3$, which can again
be matched to the rule and rewritten to $3 + 5$, and so on. The Halide TRS has been observed to fail to terminate in the past\footnote{See for example https://github.com/halide/Halide/pull/1525}, causing unbounded recursion and eventually a stack overflow in the compiler. This issue is tricky to debug, and has presumably not always reported by users, since the error is fairly opaque. To show that this type of error has been eliminated, we must prove that there is no expression in the Halide expression language that can be infinitely rewritten by some sequence of rules that form a cycle.

We can think of Halide expressions as existing in some multi-dimensional space; when an expression is rewritten by a rule, it moves from one point in that space to another. If each rule always rewrites expressions such that they move monotonically in some direction through expression space, then it will be impossible for any sequence of rules to form a cycle. These directions correspond to our intuition about why certain rules are useful: we may want to write rules that cancel expressions and make them smaller, replace expensive operations with cheaper ones, or canonicalize expressions so we can more easily prove an equivalence. We can consider each of these properties as a dimension in the expression space, in which rewrites should move expressions in the desired direction. If we can formalize this desirable ordering and show that all rewrites from one expression to another strictly obey it, then we will have a proof of termination.

We provide this formalism and prove that the Halide term rewriting system must terminate by constructing a \emph{reduction order}, a strict order with properties that ensure that, for an order $>$ and a rule $l \rightarrow_R r$, if $l > r$, then for any expression $e_1$ that matches $l$ and is rewritten by $l \rightarrow r$ into $e_2$, it must be true that $e_1 > e_2$. Crucially, this order is evaluated over rule terms, and not over all expressions that those terms may match. We take the definition of a reduction order and the next two theorems from~\cite{baader1999term}.

\begin{theorem}\label{theorem:terminates}
A term rewriting system $R$ terminates iff there exists a reduction order $>$ that satisfies $l > r$ for all $l \rightarrow_R r \in R$.
\end{theorem}

A reduction order is a strict order that must be well-founded, meaning that every non-empty set has a least element with regard to the order, to prevent infinitely descending chains. It must be \emph{compatible with $\Sigma$-operations}: for all expressions $s_1, s_2$, all $n \geq 0$, and all $f \in \Sigma$:

\[
s_1 > s_2 \implies f(t_1,...t_{i-1},s_1,t_{i+1},...,t_n) > f(t_1,...t_{i-1},s_2,t_{i+1},...,t_n)
\]

for all $i, 1 \leq i \leq n$ and all expressions $t_1,...t_{i-1},t_{i+1},...,t_n$. This property means that if a rewrite rule is used to transform a subtree in some expression $e$, the $>$ relation is preserved between the original expression $e$ and the rewritten expression $e'$. Finally, a reduction order is \emph{closed under substitution}: for all expressions $s_1, s_2$ and all substitutions $\sigma \in \mathcal{S}ub(T(\Sigma,V))$, 
$s_1 > s_2 \implies \sigma(s_1) > \sigma(s_2)$. When we match some left-hand side term $l$ to some expression $e$, we are defining a substitution for each of the variables in $l$ with some subtree in $e$; we then use that substitution to rewrite $e$ to $e'$. If our order is closed under substitutions, we know that for any expression we match to $l$, the resulting rewritten expression will obey the ordering.

Choosing a single monotonic direction in which to rewrite expressions would be overly restrictive, since there are many reasons why a given rule may be useful. The Halide TRS is used both to prove expressions true and to simplify them; when using it as a prover, we want to put both sides of an equality into some normal form, but it doesn't particularly matter what that form is. When using the TRS to simplify expressions, on the other hand, reducing the size of an expression can have performance benefits. Since we must devise an ordering that covers the existing ruleset, we make use of the following theorem:

\begin{theorem}
The lexicographic product of two terminating relations is again terminating.
\end{theorem}

Thus, our strategy in finding a reduction order to cover the handwritten ruleset is to pick an order $>_a$ such that for all rules $l \rightarrow_R r$, either $l >_a r$ or $l =_a r$. Then, we pick another order $>_b$ such that for all rules $l \rightarrow_R r$ where $l =_a r$, either $l >_b r$ or $l =_b r$. We continue in this way until a sequence of orders has been found such that for their product $>_{\times}$, $l >_{\times} r$ holds for the entire ruleset.

Many of our component orders are defined using a measure function that counts the number of particular operations or other features in a term. We say that $s > t$ when $s$ has more vector operations than $t$, then when $s$ has more division, modulo and multiplications operations, and so on. As a sample proof sketch of this flavor of order, consider an order $s_1 >_* s_2$ that holds when the number of multiplication operations is greater in $s_1$ than in $s_2$. We represent this through a measure function $|s_1|_*$ that returns the count of multiplication operations in $s_1$; since this function maps a term to a natural number, the order is clearly well-founded. The order is also compatible with $\Sigma$-operations; we compute our measure function as follows:


\[
|f(t_1,...,t_n)|_* = \sum_i^n |t_i|_* + \begin{cases} 1 & \textrm{if } f = * \\
                                                      0 & \textrm{otherwise}
                                        \end{cases}
\]

It clearly follows that given $|s_1|_* > |s_2|_*$, it must be true that:

\[
|f(t_1,...t_{i-1},s_1,t_{i+1},...,t_n)|_* > |f(t_1,...t_{i-1},s_2,t_{i+1},...,t_n)|_*
\]

To ensure the order is closed under substitution, we need to add one more constraint. Imagine a rule $x * 2 \rightarrow_R x + x$. Although there are fewer $*$ symbols in the righthand term than on the left, that would not be true for a substitution $\sigma = \{x \mapsto (z * z)\}$. We add a condition that for every variable present in $s_1$, it must occur either fewer or an equal number of times in $s_2$. With this constraint there is no possible substitution that increases the value of the measure function in $s_2$ that would not result in an increase by an equivalent or greater amount in $s_1$. This gives us the order:

\[
s_1 >_* s_2 \textrm{ iff } |s_1|_* > |s_2|_* \wedge \forall x \in \mathcal{V}ar(s_1) . |s_1|_x \geq |s_2|_x
\]

Most of the component orders in the full reduction order take the form above. These orders guarantee termination no matter what sequence rewrite rules are applied to an expression. However for part of the existing ruleset, we were obliged to take into account the order in which rules are applied in the Halide TRS algorithm.

For example, one existing rule is the canonicalization $\(c_0 - x) + y \rightarrow (y - x) + c_0$ where $c_0$ is a constant. If $y$ is also a constant, this rule forms a cycle with itself, and could not possibly obey any reduction order. Fortunately, the rule immediately before it in the TRS handles that specific case ($((c_0 - x) + c_1 \rightarrow fold(c_0 + c_1) - x)$), so by this sort of non-local reasoning we know that $y$ is not a constant, and therefore the rule strictly decreases a measure which counts the number of constants on the right-hand side of an addition.

%The handwritten ruleset had many rules that eliminated the occurrence of a variable, such as $x + x \rightarrow_R x * 2$. It seems natural to define an order based on the measure function $|s_1|_x$, but for a substitution $\sigma = \{x \mapsto 3\}$, $|3 + 3|_x \not > |3 * 2|_x$. However, the simplifier algorithm always attempts constant folding before any other rule, so we know that the rule $x + x \rightarrow_R x * 2$ can only be invoked if $x$ is not a ground term.

%Similarly, we have several rules that factor out an occurrence of a variable and introduce the constant 0 into the expression. We define an order on the occurrences of the constant 0 by defining a measure function that takes the count of terminals or leaves in the expression and subtracts the count of the constant 0; if terms $s_1$ and $s_2$ have the same number of leaves, but more of the leaves of $s_2$ are the constant 0, then $s_1 > s_2$.

%\[
%s_1 >_0 s_2 \textrm { iff } |s_1|_{leaf} - |s_1|_0 > |s_2|_{leaf} - |s_2|_0 \wedge |s_1|_{leaf} = |s_2|_{leaf} \wedge \forall x \in \mathcal{V}ar(s_1) . |s_1|_x \geq |s_2|_x
%\]

% For the rule $\texttt{max}(x + y, x) \rightarrow_R \texttt{max}(y, 0) + x$, the order will not hold for the substitution $\sigma = \{x \mapsto 0\}$. However, we know the rule $0 + x \rightarrow_R x$ will be invoked before this one, so the rule cannot be evaluated on the expression $\texttt{max}(0 + y, 0)$.

Relying on non-local reasoning makes our order more brittle; if the simplifier algorithm were to be changed, the termination guarantee could be lost. However, we use only a small number of basic rules in this way, which are unlikely to be changed.

Besides giving a termination guarantee, the reduction order is necessary if we want to synthesizing new rewrite rules. If we do not constrain newly-synthesized rules to obey a consistent reduction order with the existing human-written ones, they form cycles with the existing rules and cause infinite recursion in the TRS. Additionally, the reduction order is the formal encoding of the types of transformations we find desirable, so the reduction order limits synthesis to rules that rewrite expressions in a useful direction.

In constructing the reduction order, we found 8 rules that contradicted a desirable ordering, and submitted patches to either delete or modify them. \jln{add reference to eval section that discusses this} With this amendment, the reduction order can be shown to hold over the entire Halide ruleset, and the guarantee of termination is complete. A full description of the reduction order is given in the appendix.

\section{Increasing Completeness: Synthesizing Rewrite Rules}
\begin{figure*}
%\includegraphics[width=1.\columnwidth,natwidth=610,natheight=642]{figures/synthesis-flow.pdf}
\includegraphics[width=1.\columnwidth]{figures/synthesis-flow.pdf}
\caption{Overall flow of the synthesis pipeline.}
\label{fig:synthesis-flow}
\end{figure*}

\begin{algorithm}[htbp]
  \SetAlgoLined
  \KwIn{$e$, an expression}
  \KwOut{$r=\{r_0,..\}$, a set of rewrite rules}
  \BlankLine\BlankLine
  \tcp{Generate candidate rules (\S\ref{sec:synthesizing-candidate-rules})}
  Generate $P=\{p_0,..,p_n\}$ patterns that match subexpressions of $e$\;
  Remove $p_i \in P$ if $p_i$ does not include a repeated wildcard\;
  $r_{cand} = \{\}$\;
  \ForEach{$p_i \in P$}{
    \tcp{First try reassociation \& applying existing rules}
    Flatten pattern into $p_{flat}$\;
    \ForEach{$p_{reassoc}$}{
      \If{\texttt{rewrite($p_{reassoc}$) $\neq$ $p_{reassoc}$}}{
          \tcp{Found a candidate rewrite}
          Append $p_i$ $\rightarrow$ \texttt{rewrite($p_{reassoc}$)} to $r_{cand}$\;
          break\;
      }
    }
    \tcp{If no candidate found yet, use CEGIS}
    \If{\texttt{superoptimize($p_i$) $\neq$ $p_i$}}{
      Append $p_i$ $\rightarrow$ \texttt{superoptimize($p_i$)}\ to $r_{cand}$\;
      break\;
    }
   }
  \tcp{Generalize constants (\S\ref{sec:generalizing-constants})}
  \ForEach{$r_i \in r_{cand}$}{
    Replace concrete constants in $r_i$ with symbolic constants $c_i$\;
    $m =$ number of non-constant wildcards in $r_i$\;
    $S$ = standard basis vectors for $Z^m \cup \vec{0}$\;
    $P_{cand} = \bigwedge\limits_{\vec{x} \in S} ( LHS(\vec{x}, \vec{c}) = RHS(\vec{x}, \vec{c}) )$\;
    \Repeat{$iter > 4$ or Z3 confirms $P_{cand}$ sufficient}{
      Add counterexample to $S$ and add new clause to $P_{cand}$\;
    }
    \If{still not sufficient}{break $P_{cand}$ into disjunctive normal form and check if any clause is sufficient\;}
    \uIf{sufficient}{Perform final filtering and add to $r$\;}
    \Else{Reject candidate\;}
  }
  \caption{\label{alg:synthesis-algorithm} Pseudocode of the synthesis algorithm.}
\end{algorithm}

The Halide term rewriting system is not complete: there
exist expressions checked by the compiler running on real-world pipelines that
the TRS cannot resolve, but which are known to be true. 
In this section, we describe a workflow for automatically augmenting the Halide
TRS using program synthesis.
A high-level view of the synthesis pipeline is shown in Figure~\ref{fig:synthesis-flow},
and Algorithm~\ref{alg:synthesis-algorithm} shows the corresponding pseudocode.
We begin by synthesizing candidate rules that contain concrete constants,
then generalize those rules by replacing concrete constants with wildcards that
match any compile-time constants, along with predicates checkable at compile-time
that encode constraints on those constants.

\subsection{Synthesizing Candidate Rules With Constants}
\label{sec:synthesizing-candidate-rules}
The aim of the synthesis pipeline is to learn new rules useful for reasoning about input expressions.
We obtain scripts from the authors of \citeauthor{Adams2019}~\cite{Adams2019} that compile
 random but realistic Halide pipelines with random schedules for
training the Halide autoscheduler, and instrument the compiler to gather our dataset.
Since the rewriter works as both a prover and a simplifier, we need a dataset that reflects both usages.
For the prover usecase,  the compiler was instrumented to print
expressions that the TRS was unable to prove true, but that fuzz
testing was not able to find to be false, a good indicator of a weak spot in the
rewriter's reasoning power. For the simplifier usecase, we capture all
integer expressions present in the final Halide IR, at the point just before 
lowering to LLVM IR. These expressions have been reduced as far as the current rewriter can, so we attempt
to learn rules that could reduce them even further. 

\paragraph{Generating LHS Patterns} For a given input expression, the first step
is to construct all left-hand-side patterns that match some portion of the expression.
We constrain the maximum length of a pattern to be five operations, to ensure fast matching
during term rewriting. At this stage, patterns preserve any constant values, and subtrees
that exactly match syntactically are assigned the same wildcard. For
example, given the expression $(a*b + c/b)$, where all nodes are non-constant,
we extract the following LHS patterns: $x + y$, $x * y$, $x / y$, $x * y + z$,
$x + y/z$, and $x*y + z/y$.  We then filter out any patterns that do not either contain
a repeated wildcard or more than one constant. Such patterns are unlikely to result in a useful general
rewrite.  Thus, of the set above, we would only consider the last pattern, since
it contains a repeated $y$.  Although the set of candidate LHS patterns is exponential
in the size of the original expression, our filtering reduces the set to a tractable
size.

\paragraph{Synthesizing Right-Hand Sides} Given a candidate left-hand side, we 
synthesize a right-hand side that is semantically equivalent and respects the reduction
order such that $\mathit{LHS} > \mathit{RHS}$.  We utilize two strategies for synthesizing
right-hand sides: reassociating and applying a known rewrite, and counter-example guided
inductive synthesis (CEGIS).

The first strategy comes from the insight that in many cases existing rewrite rules would apply
if the expression were in a different form.  Therefore, we generate all possible
reassociations and commutations of the LHS expression, by unpacking sub-expressions
into a flattened list of leaves, and then enumerating every valid binary
tree over permutations of the leaves that is semantically equivalent to the original
expression.  We then check whether an existing rule rewrites any enumerated reassociation.
When enumerating trees, we consider every reassociation and commutation
of associative and commutative node types, including addition and subtraction, max, and
min.  For non-associative/non-commutative nodes, we use the outer product of all reassociations
and commutations of their input arguments.  For example, the expression $(x/2 + y) - (x + 11)/2$
does not match an existing rewrite rule, but a reassociated version does: $y + (x/2 - (x + 11)/2)$
would be rewritten to $(-10 - (x \% 2)/2) + y)$.  Thus, we construct the candidate rewrite:
$$(x/2 + y) - (x + 11)/2 \equiv (-10 - (x \% 2)/2) + y)$$

If the first method fails, we apply CEGIS to superoptimize the left-hand side pattern.
We build a CEGIS loop on top of Z3~\cite{de2008z3}, encoding the LHS as a set of op-codes
and searching for a semantically equivalent but shorter sequence of operations, similar
to prior work in superoptimization~\cite{regehr2018superoptimization, mangpo2016superoptimization}.
We first search for a single-op sequence,
then iteratively increase the number of operations, in order to find shorter sequences
first.  If CEGIS returns a sequence that is semantically equivalent to the LHS pattern but uses fewer
operations, it becomes the candidate RHS. Rather than try to encode the complex reduction order into the synthesis query itself, we use the op-code count of the synthesized expression as an approximation for the order, and then filter out any rules that do not respect the full order as a post-process. 

\paragraph{Limitations} While Z3 is a powerful tool for powering synthesis, there are certain types of expressions 
containing division or modulo that Z3 nearly always fails to reason about during the CEGIS process. (We experimented with the SMT solvers Yices2~\cite{jovanovic2017solving} and MathSAT5~\cite{mathsat5}, but were not able to obtain appreciably better results.)
Therefore we limit the use of division and modulo in our op-codes to be division
or modulo by 2 only, and rely on the generalization step described next to
widen the set of denominators for which a rule applies.  Because of this
restriction, our synthesized rules cannot contain non-constants in denominators
or the right-hand side of a modulo.  As a result, our synthesis system cannot
construct all rules a human can.

\begin{table*}
\caption{Sample rules synthesized by our process. }
\small
\begin{tabular}{l|l|l}
LHS & RHS & Predicate \\
\hline
$(x*y) - (w + (y*z))$ & $(y*(x - z)) - w $ & \\
$x < ((y + x) + z)$ &  $0 < (y + z)$ & \\
$x < (min(z, (x - y)) + min(x, y))$ &  $false$ & \\
$((x - ((y*c0) + z)) + c1)/c0$ & $((x + (c1 - z))/c0) - y$ & \\
$select((c0 < min(x, y)), (z + min(y, 0)), z)$ &  $z$ &  $-1 \leq c0 $ \\
$min((x*c0), c1)/c0$ & $min(x, -1)$ & $((0 \leq (c0 + c1)) \wedge (1 \leq c0)) \wedge (c1 \leq -1)$ \\
\end{tabular}
\label{tab:samplerules}
\end{table*}

\subsection{Generalizing Constants}
\label{sec:generalizing-constants}

At this point in the synthesis pipeline, we have constructed a candidate rewrite
rule that contains concrete constants.  To make the rule as general as possible,
we replace each constant with a fresh \emph{symbolic constant}. A symbolic constant, unlike a variable,
 matches only terms with constant values known at compile time. We first query Z3
again to see if the equivalence still holds; if it does, the rule is valid. If
it is not, we will need to find a \textit{predicate} to guard
applications of the rule in the term rewriting system. We attempt to synthesize a predicate
such that $pred \rightarrow LHS = RHS$ and all variables appearing in the
predicate are symbolic constants.

The ideal predicate would be one which is true for all cases in which
the $LHS = RHS$ and false for all cases where $LHS \neq RHS$. More
precisely, for a vector of variables $\vec{x} \in Z^n$ and a vector of
symbolic constants $\vec{c} \in Z^m$, let $LHS(\vec{x}, \vec{c})$ be the left-hand side
of the rewrite rule, and $RHS(\vec{x}, \vec{c})$ be our synthesized
right-hand side. The ideal predicate is then exactly:

\[
P_{ideal}(\vec{c}) = \bigwedge\limits_{\vec{x} \in Z^n} ( LHS(\vec{x}, \vec{c}) = RHS(\vec{x}, \vec{c}) )
\]

This is of course not a predicate we can use in practice, because it
involves an infinite conjunction over $Z^n$. We can however, unroll
the conjunction over any finite subset $S \subset Z^n$ and use that as
our predicate:

\[
P_S(\vec{c}) = \bigwedge\limits_{\vec{x} \in S} ( LHS(\vec{x}, \vec{c}) = RHS(\vec{x}, \vec{c}) )
\]

This predicate can be evaluated at compile time (with one caveat
below). If we set $S$ to the zero vector plus the standard basis for
$Z^n$ then many of the terms in the conjunction are either trivially
true or equivalent, and the unrolled conjunction often simplifies
(using the TRS itself) to a single equality.

For example, consider the candidate rewrite rule
$x_1 < select(x_2, c_0, c_1) + x_1 \rightarrow !x_2$.  In this case,
we set $S = \{\{0, 0\}, \{1, 0\}, \{0, 1\}\}$.  If we substitute the zero
vector into $LHS(\vec{x}, \vec{c}) = RHS(\vec{x}, \vec{c})$, we get
$0 < select(0, c_0, c_1) + 0 = 1$, which simplifies to $c_1 > 0$.  Thus, for $S$,
we obtain:
$$P_S(\vec{c}) = c_1 > 0 \wedge c_1 > 0 \wedge c_0 \le 0$$
which the Halide TRS will further simplify to remove the redundant term.

Because we dropped terms from the infinite conjunction, $P_S$
is now necessary but may not be sufficient. While it does recognize all the
cases in which the rule applies, it may also capture cases in which it
should not. However, for any given choice of $S$ we can improve it by
finding one of these incorrect cases using Z3, adding the
counterexample $\vec{x}$ to $S$, and iterating this until convergence in a
CEGIS-like loop.  For our example, Z3 returns that the candidate
predicate is indeed sufficient, so we do not need to enter this loop.


If the loop terminates with Z3 definitively stating that there are
no counterexamples, then $P_S$ is necessary \emph{and} sufficient, so
we have found our ideal predicate. If the loop does not terminate
after a small number of iterations (we use four) then we simplify our
$P_s$ expression using the TRS, convert it to disjunctive normal
form \footnote{While $P_S$ is initially a conjunction of equalities,
  disjunctions appear after simplification due to rewrites such as
  $c_1 \times c_2 = 0 \rightarrow c_1 = 0 \vee c_2 = 0$}, and test
each clause in turn to see if it is a sufficient condition, if not a
necessary one. If we find one, we return this single clause. If the
loop terminates with Z3 timing out or returning ``unknown'', we return
the current $P_S$, but flag it for human attention as something that
needs a manual proof. We exclude all such cases from our experiments
below.

Our resulting $P_S(\vec{c})$ may still not be possible to evaluate at
compile-time in the case where some of the symbolic constants that
make up $c$ appear only on the right-hand side. For example, in $(x_1
+ c_1) + c_2 \rightarrow x_1 + c_3$ we have no value for $c_3$ to use
in testing a predicate or constructing the right-hand side
expression. Fortunately, using the algorithm above, the only term in
this predicate is $c_1 + c_2 = c_3$, so we can simply substitute out
$c_3$ using the constant-folded left-hand side of this equality,
giving the unpredicated rule $(x_1 + c_1) + c_2 \rightarrow x_1 +
fold(c_1 + c_2)$. In general we eliminate each symbolic constant that
only appears on the right-hand side of a rewrite rule by iterating
over the clauses in the predicate, attempting to solve each for the
symbolic constant, and substituting it for its constant-folded value
if successful. This variable elimination simplifies the predicate, so
we do it eagerly inside the predicate-synthesis loop described above.

\subsection{Final Simplification \& Filtering}

Now that we have a list of rules with predicates, we augment the list
by enumerating all variants of the left-hand side with commutative
operations reordered. For each variant we search over all commutations
and reassociations of the right-hand side, and select the one most
similar to the left-hand side. We measure similarity by serializing
the expression and computing the Levenshtein distance on the resulting
strings considering only the parentheses and the leaves. This is done
to avoid pointlessly shuffling subexpressions. Variables in each rule
are then renamed to appear in a consistent order.

For example, from a synthesized rule $y - max(x, y - z) \rightarrow min(y - x, z)$ we would generate the following two
output rules by commuting the max operation:
\[
x - max(y, x - z) \rightarrow min(z, x - y) \qquad x - max(x - y, z) \rightarrow min(y, x - z)
\]

Next, we check that the rule is not redundant. For every pair of rules
$R_1$ and $R_2$, if $R_1$ matches every left-hand side that $R_2$
matches, and the predicate for $R_1$ implies the predicate for $R_2$,
then we can discard $R_2$ as $R_1$ is at least as general. Finally, we
check that the candidate rule obeys our reduction order in order to
preserve our termination guarantee. If the candidate rule passes these
filters, and the predicate has not been flagged for human review, the
rule can be added to the TRS ruleset automatically without any human
auditing.

\section{Evaluation}

In this section we evaluate the benefits of our verifier and synthesizer in three ways. First we examine bug reports from Halideâ€™s past and evaluate whether the machinery presented in this paper could have fixed them automatically. Second, we examine cases where beta versions of our verifier and synthesizer assisted humans in both fixing bugs and also correctly making larger changes to the compiler. Third, we fuzz the compiler to mine for issues that could be fixed with new simplifier rules, and automatically fix them before they ever appear as a bug in a real program. In this way we demonstrate that this machinery would have been useful in the past, is useful in the present, and will help avoid entire classes of bugs in the future.

Note for reviewers: For the purposes of this anonymous submission, we will refer to code changes by letter, with corresponding diffs found in supplemental material. In this final version of the paper these will be replaced with github issue and pull request IDs with links to Halideâ€™s github page.

\subsection{Could the synthesizer have fixed previous bugs automatically?}

We searched through Halideâ€™s change history and selected the five pull requests that addressed issues by adding new rewrite rules to Halideâ€™s TRS. These pull requests occurred before the Halide developers started routinely using the verifier and synthesizer when changing the TRS. These can be found as diffs $\mathbb{A}$-$\mathbb{E}$ in supplemental material. Creating these rewrite rules as a human is an amount of work disproportionate to the size of the change. The author of the rules must prove them correct on paper, and a second reviewer must check their work. As we will see, bugs can slip through despite this review. 

In each case we take the test expressions committed as part of the change and feed them to our synthesizer to see if it would have produced the same rewrite rules as the humans did. In cases where humans did not check in tests for their new rules, we wrote our own. In total, across these cases humans added 24 new rules. The synthesizer generated 42, covering all but one of the human rules, while correcting and generalizing others. In cases $\mathbb{A}$, $\mathbb{C}$, and $\mathbb{E}$, the rules generated by the synthesizer are an exact match to the human-generated rules. In case $\mathbb{B}$ the synthesizer matched the human, and also output 8 commuted variants of the human rules. As an example, for the human-written rule:

\[
rewrite(max(max(x, y) + c0, x), max(x, y + c0), c0 < 0)
\]

The synthesizer produced effectively the same rule~\footnote{c0 canâ€™t possibly be zero because it would have already constant folded, so while it may appear superior at first c <= 0 is not actually more general than c0 < 0.}:

\[
rewrite(max((max(x, y) + c0), x), max((y + c0), x), (c0 <= 0))
\]

and also the variant:
\[
rewrite(max(x, (max(x, y) + c0)), max(x, (y + c0)), (c0 <= 0))
\]

Case $\mathbb{D}$ is the most interesting. It contains four rules involving comparisons of min and max operations. What happened for each was identical, so we will only discuss the min rules. The first rule is:

\[
rewrite(min(x, c0) < min(x, c1) + c2, false, c0 >= c1 + c2)
\]

This rule is incorrect (consider c0 = c2 = 1, x = c1 = 0). It can be fixed by adding the term c2 <= 0 to the predicate. The synthesizer output the correct version of this rule, along with two generalizations of it:

\begin{align*}
&rewrite((min(x, c0) < (min(x, c1) + c2)), false, ((c2 <= 0) \&\& ((c1 + c2) <= c0))) \\
&rewrite((min(x, c0) < (min(x, y) + c1)), (fold((c0 - c1)) < min(x, y)), (c1 <= 0)) \\
&rewrite((min(x, c0) < (min(y, x) + c1)), (fold((c0 - c1)) < min(y, x)), (c1 <= 0))
\end{align*}

The second human rule was:
\[
rewrite(min(x, c0) < min(x, c1), false, c0 >= c1) 
\]
The synthesizer found a more general rule, along with three other commuted variants of it:
\[
rewrite((min(x, y) < min(x, z)), (y < min(x, z)))
\]
Any expression which matches the human-written rule would also match the synthesized one. However, unlike the human-written rule, the synthesized one does not simplify to the constant â€œfalseâ€ in a single step. However after applying this rule to the case considered by the human, we get $c0 < min(x, c1)$ where $c0 \geq c1$. The simplifier then reduces this to â€œfalseâ€ in a second step, so the human-written rule becomes unnecessary. The synthesizer considered the human-written rule, but discarded it as less general than the one above.

Case $\mathbb{D}$ also included the rewrite rule: 
\[
rewrite(x \% x, 0)
\]
which was the sole rule the synthesizer could not generate, as we did not include modulo by non-constants in our CEGIS interpreter.

With this one exception, across these five code changes the synthesizer generated more general, more correct rules than the humans, and would clearly have been a useful assistant to the Halide developers if they had had it at the time.

\aba{TODO: Could we have synthesized the entire simplifier? Currently looks like we can synthesize about 30\% of the rules?
}

\subsection{Practical uses of the synthesizer and verifier}

We now go beyond the counterfactuals above and discuss cases where beta versions of the synthesizer and verifier assisted Halide developers in fixing new issues and making larger changes to the compiler. 

The first use of verification was in the original construction of the TRS. Before it was committed, we applied the earliest version of our verification pipeline, and discovered three incorrect rewrite rules, listed in Table~\ref{tab:verfirstround}.

\begin{table*}
\caption{Rules corrected through the first round of verification.}
\begin{tabular}{|l|l|}
\hline \\
Wrong &  rewrite((x * c0) / c1, x / fold(c1 / c0), c1 \% c0 == 0 \&\& c1 > 0) \\
Counterexample & c0 = -1, c1 = 2, x = 1\\
Fixed & rewrite((x * c0) / c1, x / fold(c1 / c0), c1 \% c0 == 0 \&\& c0 > 0 \&\& c1 / c0 != 0)\\
\hline \\
Wrong & rewrite(((x + c0)/c1)*c1 - x, x \% c1, c1 > 0 \&\& c0 + 1 == c1)\\
Counterexample & c0 = 2, c1 = 3, x = -5\\
Fixed &  rewrite(((x + c0)/c1)*c1 - x, (-x) \% c1, c1 > 0 \&\& c0 + 1 == c1)\\
\hline \\
Wrong & rewrite(x - ((x + c0)/c1)*c1, -(x \% c1), c1 > 0 \&\& c0 + 1 == c1)\\
Counterexample & c0 = 2, c1 = 3, x = -5\\
Fixed & rewrite(x - ((x + c0)/c1)*c1, ((x + c0) \% c1) + fold(-c0), c1 > 0 \&\& c0 + 1 == c1) \\
\hline
\end{tabular}
\label{tab:verfirstround}
\end{table*}

While these bugs were found automatically the fixes were performed by hand, as the synthesis pipeline did not yet exist. There are four more recent practical uses of the synthesizer and verifier. The diffs are available as $\mathbb{F}$ through $\mathbb{I}$ in supplemental material. 

In case $\mathbb{F}$ a Halide developer encountered expressions that seemed like they could be simpler while working on real code, and rather than inventing a rule from scratch searched the logs of the synthesizer project for a known-correct synthesized rule that handled the case in question. This added four new rules that are variants on:
\[
rewrite((max(y, z) < min(x, y)), false)
\]
In case $\mathbb{G}$ a developer wrote 24 new rules, proving them on paper, and then checked their work by resynthesizing the predicates using the synthesizer to ensure the synthesized predicates agreed with the humanâ€™s, and that those predicates are as broad as possible. Here the synthesis machinery served as a reviewer of rules rather than an author. Eight of these rules were in fact manual rederivations of the synthesizerâ€™s output on case $\mathbb{D}$ above. The remaining 16 are generalizations that add constant terms. One example:
\[
rewrite(min(y + c0, z) < min(y, x), min(z, y + c0) < x, c0 < 0)
\]

\subsubsection{Redefining division by zero}

Case $\mathbb{H}$ is a change to the semantics of Halide that may not have even been attempted without the verifier. In this change, Halide defined division or modulo by zero to evaluate to zero, instead of being undefined behavior, in response to an issue discovered by \aba{(cite Alexâ€™s paper on formalizing Halide)}. Existing tests and real uses of Halide were useless as a test of this change, as it was all carefully written to never divide by zero! Within the TRS, this change required rechecking every rewrite rule that involves the division and modulo operators. Whereas previously each rule assumed that a denominator on the LHS could not be zero, now we must ensure that the rule was still correct in the case where a denominator is zero, or constrain the rule to only trigger when the denominator is known to be non-zero. This was done by encoding the new semantics into the verifier, and reverifying all rules. Because division and modulo is involved, these rules cannot always be mechanically verified. 141 rules were reverified with a human in the loop by revisiting and modifying existing coq proofs. In this process, 44 existing rules were found to be incorrect in the new semantics and fixed. Two of them were in fact not related to division, but were instead the first discovery of the bugs injected in case $\mathbb{D}$ above. The remaining 42 rules were modified to only trigger when the denominator was known to be non-zero, either by adding a predicate to the rule, or by exploiting the TRSâ€™s ability to track constant bounds and alignment of subexpressions. Three examples of newly-incorrect rules were:

\begin{align*}
& rewrite((x/y)*y + x\%y, x) \\
& rewrite(-1 / x, select(x < 0, 1, -1)) \\
& rewrite((x + y)/x, y/x + 1)
\end{align*}

The first was modified to:
\[
rewrite((x/c0)*c0 + x\%c0, x, c0 != 0)
\]
and the other two were constrained to only trigger when the denominator is known to be non-zero via other means.

\subsubsection{Stricter input bounds checking}

Halide endeavors to be a safe language, meaning that certain things are checked at compile time or runtime rather than being undefined behavior. Halide has always checked that input buffers are large enough such that no value in any output buffer has a data dependency on anything out-of-bounds in an input. It does this checking using symbolic interval arithmetic applied to index expressions. Consider for example the two-stage algorithm that performs a three-tap blur on a 1D input:

\begin{align*}
& \texttt{tmp(x) = input(x) + input(x + 1) + input(x + 2)} \\
& \texttt{output(x) = tmp(x) / 3}
\end{align*}

When compiling this code, Halide injects an assertion that the input is two pixels wider than the output at the beginning of the generated code, so for an output of size 5, the input must be at least of size 7. 


However, Halide scheduling directives can expand the bounds computed of a stage. If we apply the schedule:
\[
\texttt{tmp.compute_root().vectorize(x, 8)}
\]
Then in order to compute 5 values of the output, we must compute an entire 8-lane vector of the intermediate tmp (due to the vectorize directive). Three lanes of this vector will go unused, but they are still computed, which means 10 distinct values are loaded from the input. While none of these three additional values loaded can possibly affect an output pixel, the load still occurs, and could for example cause a segmentation fault if the addresses happen to cross a page boundary. \aba{if we include this detail at all, it might work better as a figure that shows the true and spurious data dependencies}

In case $\mathbb{I}$, Halide was changed such that instead of asserting that no output value has a dependency on any out-of-bounds input values, it now asserts the stricter condition that no out-of-bounds loads occur on the input, even if those loaded values cannot possibly affect an output.

This is a harder thing for the compiler to check. True data dependencies can be inferred from the algorithm alone, which typically uses simple axis-aligned affine index expressions on which interval arithmetic is exact. Inferring a bounding box over the values actually loaded also involves the schedule. In practice this is done by lowering the code to an imperative form, and then analyzing the accesses to input buffers in the lowered code. This is prone to false positives. In cases where correlated subexpressions cause interval arithmetic to overestimate the size of the input required, the generated program now incorrectly throws an error that the code would read out of bounds, when in fact it would not.

In case $\mathbb{I}$, the Halide developers ameliorate this in part by minimizing the number of these non-monotonic expressions using aggressive simplification. In total 59 new rewrite rules were added as part of this change. Eight of these were synthesized automatically by copy-pasting a non-monotonic expression from a bug report into the synthesis machinery. Another 28 came from generalizing those rules by hand and then verifying the result. Twenty more were written by hand and then verified. Finally, there were three rules that could not be verified, because they involve the interaction of div and mod. They were:

\begin{align*}
& rewrite((x + y) / c0 - (x + z) / c0, ((x \% c0) + y) / c0 - ((x \% c0) + z) / c0) \\
& rewrite(x / c0 - (x + z) / c0, 0 - ((x \% c0) + z) / c0) \\
& rewrite((x + y) / c0 - x / c0, ((x \% c0) + y) / c0)
\end{align*}

These rules incidentally do not follow the reduction order. They were added outside of the TRS in another part in the compiler for which the same concerns about termination do not apply because rules are not applied to the output of other rules.

During this process a large number of bugs were found in human-written early versions of these rules. We found that with the verifier and synthesizer in hand, humans work quickly and rely on the machinery to catch their mistakes.

Generalizing from these four cases, we have found that having the verifier and synthesizer available as tools reduces the number of bugs committed, uncovers and fixes old bugs, and helps developers work more quickly by not only mechanizing correctness checking but also by synthesizing correct code. We also found that the guarantees these tools provide mean that the developers can make large changes to the compiler such as those above with confidence. Anecdotally, developers also report that eliminating these classes of bug makes triaging new issues simpler, because they could now not possibly be due to an incorrect simplifier rule or an infinite loop in the simplifier.

\subsection{Finding and fixing undiscovered issues automatically}
\label{ssec:compilationspeed}
The cases above all concern fixing existing problems while not introducing new ones. By verifying that the simplifier is correct and that the rules are cycle-free, we also remove two entire classes of future bugs. For reference, over the life of the Halide project there have been 14 pull requests that fix incorrect simplifier rules, and 3 pull requests that modify rules in order to avoid cycles. 

Even with these classes of bug now eliminated, there are still unknown unknowns. There are probably compilations in which the Halide compiler does a bad job in a way that a new simplifier rule or two could fix, but we donâ€™t know what they are because no human has encountered them yet (or more likely, no human has been sufficiently motivated to submit a bug report for them yet). 

We probed the compiler for issues of this nature using fuzzing. We selected the 12 most complex example applications in the open source repository, and generated 64 random schedules for each using the autoscheduler~\cite{Adams2019}, which can be configured to generate random likely-good schedules. This produced 768 separate compilations.

We also instrumented most of the Halide code at company X (anonymous for submission), for an additional 5032 compilations, this time using the original human-written schedules. Note that this is qualitatively different to randomly-generated schedules. We do not expect Halide users at company X would check in code that behaves poorly due to an issue with the compiler.

From each compilation we log all integer expressions found that are non-monotonic with respect to a containing loop, and all failed proof attempts made during compilation. That is, we log all boolean expressions passed to the TRS in the hope that they will reduce to the constant â€œtrueâ€ so that some optimization can correctly be performed.

From this combined set of expressions we synthesize new rewrite rules, add all rules found back to the compiler, and rerun all compilations to gather new expressions, repeating this process until convergence. In total we created 4127 new rewrite rules in this way, more than quadrupling the number of rules in the TRS, from a final corpus of roughly one hundred thousand unique expressions. For some examples, refer to Table \ref{tab:samplerules}.

We then generate a fresh set of 256 random schedules per application (to avoid testing on our training set), and compile and run all the generated code, looking for any compilations which behave significantly differently between the baseline condition (compiled using unmodified Halide) and the test condition (compiled with the 4127 additional rewrite rules). The interesting findings are summarized below.

\paragraph{Adding new rules lowers peak memory usage by up to 50\%}
Halide sizes internal allocations using symbolic interval arithmetic,
which as described above, is prone to overestimating bounds when
expressions do not either monotonically increase or decrease with
respect to some containing loop. By extending the TRS, we
automatically fix 197 cases where this kind of error increases the
peak memory usage of an application by more than 10\%, including one
case where the increase was more than a gigabyte. This represents
nearly 6\% of all compilations tested. We believe this captures a
widespread problem, as instances of overallocation are a common source
of complaint from users. See Figure~\ref{fig:peakmemoryhistogram} for
the full distribution.

\paragraph{Term rewriting systems written without verification have bugs}
On our initial run of this experiment, 55 compilations (1.6\%) crashed
at runtime with memory corruption errors in the baseline condition (no
new rules added). We traced this to an incorrect transformation in a
separate, unverified TRS in the Halide compiler (the ``solver''). This
bug had existed for four years, but had only recently become an
important code path due to change $\mathbb{I}$ mentioned above. The
transformation was $min(x - y, x - z) \rightarrow x - min(y, z)$,
which should be $min(x - y, x - z) \rightarrow x - max(y, z)$. If this
secondary TRS had been written using verification, this bug would
never have been introduced. We intend to formalize and verify this
secondary TRS next to flush out any other bugs lurking therein.

\paragraph{The TRS scales well with the number of rules}
Remarkably, quadrupling the size of the TRS increased compile times by
only 0.3\%. On further examination we found that any increase in
compile time spent inside the TRS was balanced by a decrease in
compile time in later phases of compilation. Stronger simplification
reduces the size and complexity of the program, which makes every
subsequent compiler pass faster. TODO: how many extra cycles per rule
added spent in TRS?

We did not find significant effects on runtime of the generated code or code size. We also found no significant differences on any metrics within the company X corpus. This may be because the random schedules we generate are especially complex compared to human-written ones, or simply because humans donâ€™t commit code that causes the compiler to misbehave.

While adding this many new rules does not come at any significant cost we could measure to users of Halide, it increases the compile-time and code size of the compiler itself, so we do not propose adding all of these rules to the TRS permanently. Our next task is to triage the rules to select only those that are necessary to gain the peak memory reductions described above.

These results show that having verification and synthesis available as a tool for compiler authors fixes existing bugs, prevents entire classes of new bugs, and even helps compiler writers change the semantics of their language with confidence. We intend to continue to use verification and synthesis to maintain the TRS, and based on this experience plan to expand its use elsewhere in the compiler.



\label{sec:eval-correctness}
\begin{table*}

\caption{Incorrect rules found during verification.}
\begin{tabular}{l|l|l}
Incorrect rule & Counterexample & Tool used \\
\hline
$((x + c0)/c1)*c1 - x \rightarrow_R x \bmod c1 \textrm{ if } c1 > 0 \wedge c0 + 1 == c1$ & x := -2, c0 := 2, c1 := 3 & Z3 \\
$x - ((x + c0)/c1)*c1 \rightarrow_R -(x \bmod c1) \textrm{ if } c1 > 0 \wedge c0 + 1 == c1$ & x := -2, c0 := 2, c1 := 3 & Z3 \\
$min(x, c0) < min(x, c1) + c2 \rightarrow_R \textrm{false if } c0 >= c1 + c2)$ & x := 0, c0 := 0, c1 := -1, c2 := 1 & Z3 \\
$max(x, c0) < max(x, c1) + c2 \rightarrow_R \textrm{false if } c0 >= c1 + c2$ & x := 0, c0 := 2, c1 := 1, c2 := 1 & Z3 \\
\end{tabular}
\label{tab:incorrectrules}
\end{table*}

\section{Limitations \& Future Work}
For this work, we considered only the subset of the term rewriting system that
is used to prove properties over infinite integers; the full TRS includes rules
for simplifying expressions with floating point values as well as rules for
fixed-bitwidth integers.  As a result, we do not consider cases where the TRS
must also reason about whether overflow can occur.  Extending our improvements
and automation to such rules could be done in future work.

One major limitation of the synthesis process we use is that our oracle
for rules, Z3, often cannot reason about expressions with divisions or modulo
where the right operand is a variable.  Though we work around this to
synthesize rules with generalized predicates on right hand side constants,
the overall synthesis machinery cannot generalize these to non-constants.
Extending the synthesizer may be more tractable for rules that operate
on integers with finite bitwidths.

The Halide TRS is used both to prove expressions true or false and to
simplify expressions to more easily optimizable forms, but these two use cases
are not always aligned. One could imagine two separate rewriters with
different rulesets and reduction orders. One interesting direction for future
work might be to use the synthesizer to create these two rulesets, which would
otherwise be a very human-effort intensive task.

Although we do not currently synthesize rules that contain vector operators,
they are not incompatible with our approach. Since our reduction of Halide
expressions to SMT2 formulas models vector expressions as integers, we would need
to add a typechecking step to ensure correctness in the synthesis process, which
we leave as future work.

The priority in which rules are considered for matching clearly can have
performance implications, but evaluation and tuning is left as future work, and
may require modifying our ordering or creating different orderings for different
uses of the term rewriting system.

Finally, while enhancing the Halide TRS was not able to improve the performance of the mature Halide compiler, we suspect this may not be the case for other, less well-exercised compiler backends. In future work we plan to use this technique to enhance the ruleset used to compile Halide applications to GPU code or to the Hexagon ISA.

\section{Related Work}
Perhaps the closest related work is the Alive project~\cite{lopes2015alive,menendez2017aliveinfer}.
The fundamental difference between Alive and this
work is that Alive works within the decidable theory of bitvectors, while
(because of Halide semantics) we must use the undecidable theory of integers;
this constraint is the major reason for many of our design choices. In addition:
Alive verifies optimizations (and Alive-Infer synthesizes preconditions), while
we synthesize rewrites and predicates, as well as verify them; Alive must contend
with more types of undefined behavior, which the Halide expression
language need not consider; and Alive uses a simple reduction order in
which all optimizations reduce program size, while our termination proof is more
complex. We originally tried synthesizing rule predicates with the approach used
by Alive-Infer but were not successful: using Z3 to generate positive and
negative examples did not scale for us, requiring seconds to minutes per query
due to the underlying theory of integers.  Moreover, queries with
division/modulo over the integers often did not work at all, simply returning
``unknown.''

Most recently, leveraging a TRS along with synthesized rules has been applied
to optimizing fully-homomorphic encryption (FHE) circuits~\cite{lee2020fhe}.  This
system synthesizes equivalent circuits with lower cost from small example circuits,
then applies the equivalences in a divide-and-conquer manner; the rewrites do
not contain preconditions. In further contrast to our work, the domain of FHE yields a
simple cost function (the depth of nested multiplications in the circuit), and
the underlying theory of boolean circuits is decidable.


An \textit{equivalence graph} or egraph, as introduced by Nelson~\cite{nelson1980techniques}, is a data structure used to compute applications of the rules of a term rewriting system. The algorithm builds up equivalence classes by successively applying all rules to all expressions within those classes, then queries to see if two expressions are equivalent by checking if they are present in the same class. Like our algorithm, it does not backtrack, but the egraph can require significant amounts of memory, which our algorithm avoids.

Herbie~\cite{panchekha2015automatically}, a tool for improving the accuracy of floating point arithmetic, uses an egraph term rewriting system made up of a small library of axioms to find repairs once a fault has been localized. Herbie assures termination by bounding the number of rewrites their system may apply, and achieves good performance by pruning the expression search space and applying rewrites only to particular expression nodes. 

Besides the closely-related projects described above, program synthesis has been applied to term rewriting systems in several domains. Swapper~\cite{singh2016swapper} synthesizes a set of rewrite rules to transform SMT formulas into forms that can be more easily solved by theory solvers, similar to the use of the Halide TRS as a simplifier, using the SKETCH tool. Butler 2017~\cite{butler2017synthesizing} learns human-interpretable strategies (essentially rewrite rules) for puzzle games such as Sudoku or Nonograms and Butler 2018~\cite{butler2018framework} finds tactics for solving K-12 algebra problems, both using a CEGIS loop similar to our synthesis process. None of these address termination, although Swapper likely screens out non-terminating rulesets through its autotuning step. The Butler works both focus on synthesizing small, highly general rulesets that are similar to human rewriting strategies, unlike the Halide TRS which tolerates very large rulesets.

Superoptimization, a process of finding a shorter or more desirable program that is semantically equivalent to a larger one, is similar to our work synthesizing right-hand side terms for candidate LHSs. STOKE~\cite{schkufza2013stochastic} uses Monte Carlo Markov Chain sampling to explore the space of x86 assembly programs, while Phothilimthana et al.~\cite{phothilimthana2016scaling} describes a cooperative superoptimizer that searches for better programs using multiple techniques in a way that allows them to learn from each other.  Souper~\cite{sasnauskas2017souper} is a recent synthesis-based superoptimizer for LLVM, which was used as the basis for evaluating the effectiveness of Alive-Infer's precondition synthesis.

SMT solvers seek to achieve practical results in theoretically challenging problems, such as the theory of non-linear integer arithmetic. For example, Jovanovic~\cite{jovanovic2017solving} describes a satisfiability procedure for NLIA that is effective in practice and implements it in the Yices2 solver. In this work we leverage Z3's NLIA solving abilities and extend synthesis to include certain types of non-linear expressions by using constants as operands and later generalizing.

\section{Conclusion}
In this work, we improved the Halide term rewriting system by applying formal
techniques to verify rewrite rules and to ensure termination.  In this process,
we discovered \NumRulesFixed incorrect rules and \NumPredicatesRelaxed cases
where we could make rules more general, as well as \NumOrderingProblems rules
that could potentially cause termination problems.  We additionally built
an automated synthesis-based pipeline for constructing new rewrite rules from
proof failures, augmenting the compiler with \NumRulesSynthesized synthesized
rewrite rules and demonstrating that our system can be used to automatically fix
user-reported compiler bugs.

Our improvements guarantee the soundness of the term rewriting system
and increase its robustness and coverage, while (counterintuitively)
decreasing total compile times. There are no significant costs or
downsides we could identify. Indeed, augmenting this part of the
Halide compiler using verification and synthesis seems to constitute a
free lunch, and we expect to be able to make a strong case for the
inclusion of this work into the compiler.


%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}


%% Bibliography
\bibliography{bib}

\end{document}
